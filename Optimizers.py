import numpy as npclass Optimizer:    def __init__(self, learning_rate=None, name=None):        self.learning_rate = learning_rate        self.name = name    def config(self, layers):        pass    def optimize(self, idx, layers: list, grads: dict, *args):        passclass GD(Optimizer):    def __init__(self, **kwargs):        super().__init__(**kwargs)    def config(self, layers):        pass    def optimize(self, idx, layers, grads, epoch_idx, steps):        updated_W = - self.learning_rate * grads[f'dW{idx}']        updated_b = - self.learning_rate * grads[f'db{idx}']        layers[idx].W += updated_W        layers[idx].b += updated_bclass SGDM1(Optimizer):    def __init__(self, beta, **kwargs):        super().__init__(**kwargs)        self.beta = beta        self.v = dict()    def config(self, layers):        for i in layers.keys():            self.v[f'W{i}'] = 0            self.v[f'b{i}'] = 0    def optimize(self, idx, layers, grads, epoch_idx, steps):        self.v[f'W{idx}'] = self.beta * self.v[f'W{idx}'] + (1 - self.beta) * grads[f'dW{idx}']        self.v[f'b{idx}'] = self.beta * self.v[f'b{idx}'] + (1 - self.beta) * grads[f'db{idx}']        updated_W = - self.learning_rate * self.v[f'W{idx}']        updated_b = - self.learning_rate * self.v[f'b{idx}']        layers[idx].W += updated_W        layers[idx].b += updated_bclass SGDM2(Optimizer):    def __init__(self, ini_mo, fin_mo, demon, ini_beta, **kwargs):        super().__init__(**kwargs)        self.ini_mo = ini_mo        self.fin_mo = fin_mo        self.demon = demon        if self.demon:            self.ini_beta = ini_beta        self.m = dict()    def config(self, layers):        for i in layers.keys():            self.m[f'W{i}'] = 0            self.m[f'b{i}'] = 0    def optimize(self, idx, layers, grads, epoch_idx, steps):        updated_mo = min(((self.ini_mo * 1.2) ** (epoch_idx - 1)), self.fin_mo)        if self.demon:            frac = 1 - (epoch_idx/self.epoch)            updated_mo = (self.ini_beta * frac) / ((1 - self.ini_beta) + (self.ini_beta * frac))        self.m[f'W{idx}'] = self.m[f'W{idx}'] * updated_mo - self.learning_rate * grads[f'dW{idx}']        self.m[f'b{idx}'] = self.m[f'b{idx}'] * updated_mo - self.learning_rate * grads[f'db{idx}']        layers[idx].W += self.m[f'W{idx}']        layers[idx].b += self.m[f'b{idx}']class Nesterov(SGDM2):    def __init__(self, **kwargs):        super().__init__(**kwargs)    def optimize(self, idx, layers, grads, epoch_idx, steps):        updated_mo = min(((self.ini_mo * 1.2) ** (epoch_idx - 1)), self.fin_mo)        if self.demon:            frac = 1 - (epoch_idx / self.epoch)            updated_mo = (self.ini_beta * frac) / ((1 - self.ini_beta) + (self.ini_beta * frac))        prev_m_W = np.array(self.m[f'W{idx}'], copy=True)        prev_m_b = np.array(self.m[f'b{idx}'], copy=True)        self.m[f'W{idx}'] = self.m[f'W{idx}'] * updated_mo - self.learning_rate * grads[f'dW{idx}']        self.m[f'b{idx}'] = self.m[f'b{idx}'] * updated_mo - self.learning_rate * grads[f'db{idx}']        updated_W = - updated_mo * prev_m_W + (1 + updated_mo) * self.m[f'W{idx}']        updated_b = - updated_mo * prev_m_b + (1 + updated_mo) * self.m[f'b{idx}']        layers[idx].W += updated_W        layers[idx].b += updated_bclass Adagrad(Optimizer):    def __init__(self, epsilon, **kwargs):        super().__init__(**kwargs)        self.epsilon = epsilon        self.V = dict()    def config(self, layers):        for i in layers.keys():            self.V[f'W{i}'] = 0            self.V[f'b{i}'] = 0    def optimize(self, idx, layers, grads, epoch_idx, steps):        self.V[f'W{idx}'] += grads[f'dW{idx}'] ** 2        self.V[f'b{idx}'] += grads[f'db{idx}'] ** 2        updated_W = (- self.learning_rate * grads[f'dW{idx}']) / (np.sqrt(self.V[f'W{idx}'] + self.epsilon))        updated_b = (- self.learning_rate * grads[f'db{idx}']) / (np.sqrt(self.V[f'b{idx}'] + self.epsilon))        layers[idx].W += updated_W        layers[idx].b += updated_bclass RMSProp(Optimizer):    def __init__(self, decay_rate, epsilon, **kwargs):        super().__init__(**kwargs)        self.decay_rate = decay_rate        self.epsilon = epsilon        self.cache = dict()    def config(self, layers):        for i in layers.keys():            self.cache[f'W{i}'] = 0            self.cache[f'b{i}'] = 0    def optimize(self, idx, layers, grads, epoch_idx, steps):        self.cache[f'W{idx}'] = self.decay_rate * self.cache[f'W{idx}'] + (1 - self.decay_rate) * grads[f'dW{idx}'] ** 2        self.cache[f'b{idx}'] = self.decay_rate * self.cache[f'b{idx}'] + (1 - self.decay_rate) * grads[f'db{idx}'] ** 2        updated_W = (- self.learning_rate * grads[f'dW{idx}']) / (np.sqrt(self.cache[f'W{idx}'] + self.epsilon))        updated_b = (- self.learning_rate * grads[f'db{idx}']) / (np.sqrt(self.cache[f'b{idx}'] + self.epsilon))        layers[idx].W += updated_W        layers[idx].b += updated_bclass Adam(Optimizer):    def __init__(self, beta1, beta2, epsilon, demon, weight_decay, gamma, decay_rate, **kwargs):        super().__init__(**kwargs)        self.beta1 = beta1        self.beta2 = beta2        self.epsilon = epsilon        self.demon = demon        self.weight_decay = weight_decay        if self.weight_decay:            self.gamma = gamma            self.decay_rate = decay_rate        self.V = dict()        self.S = dict()    def config(self, layers):        for i in layers.keys():            self.V[f'W{i}'] = 0            self.V[f'b{i}'] = 0            self.S[f'W{i}'] = 0            self.S[f'b{i}'] = 0    def optimize(self, idx, layers, grads, epoch_idx, steps):        if self.demon:            frac = 1 - (epoch_idx/self.epoch)            self.beta1 = self.beta1 * (frac/((1 - frac) + (self.beta1 * frac)))        else:            self.beta1 = self.beta1        self.V[f'W{idx}'] = self.beta1 * self.V[f'W{idx}'] + (1 - self.beta1) * grads[f'dW{idx}']        self.V[f'b{idx}'] = self.beta1 * self.V[f'b{idx}'] + (1 - self.beta1) * grads[f'db{idx}']        self.S[f'W{idx}'] = self.beta2 * self.S[f'W{idx}'] + (1 - self.beta2) * (grads[f'dW{idx}'] ** 2)        self.S[f'b{idx}'] = self.beta2 * self.S[f'b{idx}'] + (1 - self.beta2) * (grads[f'db{idx}'] ** 2)        V_W_corrected = (self.V[f'W{idx}']) / (1 - self.beta1 ** steps)        V_b_corrected = (self.V[f'b{idx}']) / (1 - self.beta1 ** steps)        S_W_corrected = (self.S[f'W{idx}']) / (1 - self.beta2 ** steps)        S_b_corrected = (self.S[f'b{idx}']) / (1 - self.beta2 ** steps)        updated_W = (- self.learning_rate * V_W_corrected) / (np.sqrt(S_W_corrected + self.epsilon))        updated_b = (- self.learning_rate * V_b_corrected) / (np.sqrt(S_b_corrected + self.epsilon))        if self.weight_decay:            self.gamma = self.gamma * self.decay_rate ** (int(epoch_idx / 5))            updated_W = (- self.learning_rate * V_W_corrected) / (np.sqrt(S_W_corrected + self.epsilon) + self.gamma * layers[idx].W)            updated_b = (- self.learning_rate * V_b_corrected) / (np.sqrt(S_b_corrected + self.epsilon) + self.gamma * layers[idx].b)        layers[idx].W += updated_W        layers[idx].b += updated_b