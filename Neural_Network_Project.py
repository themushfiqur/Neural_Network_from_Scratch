import numpy as npfrom statistics import meanimport Optimizers as OPTimport matplotlib.pyplot as pltimport matplotlib.patches as mpatchesclass Layer:    def __init__(self, hidden_units: int, activation: str=None):        self.hidden_units = hidden_units        self.activation = activation        self.W = None        self.b = None    def initialize_parameter(self, n_in, hidden_units):        np.random.seed(42)        self.W = np.random.randn(n_in, hidden_units) * np.sqrt(2/n_in)        self.b = np.zeros((1, hidden_units))    def ini_forward(self, X):        self.input = np.array(X, copy=True)        if self.W is None:            self.initialize_parameter(self.input.shape[-1], self.hidden_units)        self.Z = X @ self.W + self.b        if self.activation is not None:            self.A = self.activation_function(self.Z)            return self.A        return self.Z    def activation_function(self, Z, derivative=False):        if self.activation == 'ReLU':            if derivative:                return self.dReLU(Z)            return self.ReLU(Z)        if self.activation == 'Sigmoid':            if derivative:                return self.dSigmoid(Z)            return self.Sigmoid(Z)        if self.activation == 'Softmax':            if derivative:                return self.dSoftmax(Z)            return self.Softmax(Z)    @staticmethod    def ReLU(Z):        return np.maximum(0, Z)    @staticmethod    def dReLU(Z):        return np.where(Z<=0, 0, 1)    @staticmethod    def Sigmoid(Z):        return (1 / 1 + np.exp(-Z))    @staticmethod    def dSigmoid(Z):        return (Layer.Sigmoid(Z) * (1 - Layer.Sigmoid(Z)))    @staticmethod    def Softmax(Z):        exps = np.exp(Z - np.max(Z, axis=1, keepdims=True))        return exps / np.sum(exps, axis=1, keepdims=True)    @staticmethod    def dSoftmax(Z):        exps = np.exp(Z - np.max(Z, axis=1, keepdims=True))        return (exps / np.sum(exps, axis=0) * (1 - (exps / np.sum(exps, axis=0))))    def __repr__(self):        return str(f'''Hidden Units = {self.hidden_units}; Activation = {self.activation}''')class Model:    def __init__(self):        self.layers = dict()        self.cache = dict()        self.grads = dict()    def add(self, layer):        self.layers[len(self.layers) + 1] = layer    def set_config(self, epoch, learning_rate, optimizer):        self.epoch = epoch        self.learning_rate = learning_rate        self.optimizer = optimizer        if not not self.optimizer:            self.optimizer.config(self.layers)            self.optimizer.epoch = self.epoch            self.optimizer.learning_rate = self.learning_rate    def forward(self, X):        for idx, layer in self.layers.items():            X = layer.ini_forward(X)            self.cache[f'W{idx}'] = layer.W            self.cache[f'Z{idx}'] = layer.Z            self.cache[f'A{idx}'] = layer.A        return X    def backward(self, Y):        last_layer_idx = max(self.layers.keys())        m = Y.shape[0]        for i in reversed(range(1, last_layer_idx + 1)):            if i == last_layer_idx:                self.grads[f'dZ{i}'] = self.cache[f'A{i}'] - Y            else:                self.grads[f'dZ{i}'] = self.grads[f'dZ{i + 1}'] @ self.cache[f'W{i + 1}'].T * self.layers[i].activation_function(self.cache[f'Z{i}'], derivative=True)            self.grads[f'dW{i}'] = (1 / m) * self.layers[i].input.T @ self.grads[f'dZ{i}']            self.grads[f'db{i}'] = (1 / m) * np.sum(self.grads[f'dZ{i}'], axis=0, keepdims=True)            assert self.grads[f'dW{i}'].shape == self.cache[f'W{i}'].shape    def update_parameter(self, epoch_idx, steps):        for i in self.layers.keys():            self.optimizer.optimize(i, self.layers, self.grads, epoch_idx, steps)    def prediction_acc(self, X, Y):        pred = self.forward(X)        acc = np.argmax(pred, axis=1) == np.argmax(Y, axis=1)        mod_acc = float('{:.2f}'.format((list(acc).count(True) / len(acc)) * 100))        return mod_acc    def fit(self, x_train, y_train, x_test=None, y_test=None, batch_size=None):        training_losses = []        validation_losses = []        training_accuracy = []        validation_accuracy = []        for i in range(1, self.epoch + 1):            batches = self.create_batch(x_train, y_train, batch_size)            epoch_loss = []            steps = 0            for x, y in batches:                steps += 1                prediction = self.forward(x)                loss = self.logistic_loss(y, prediction)                epoch_loss.append(loss)                self.backward(y)                self.update_parameter(i, steps)            loss = float('{:.3f}'.format(np.sum(epoch_loss)/len(epoch_loss)))            training_losses.append(loss)            val_pred = self.forward(x_validation)            val_loss = self.logistic_loss(y_validation, val_pred)            validation_losses.append(val_loss)            training_pred = self.forward(x_train)            acc_pred = np.argmax(training_pred, axis=1) == np.argmax(y_train, axis=1)            train_acc = float('{:.2f}'.format((list(acc_pred).count(True)/len(acc_pred)) * 100))            training_accuracy.append(train_acc)            validation_pred = self.forward(x_validation)            acc_pred = np.argmax(validation_pred, axis=1) == np.argmax(y_validation, axis=1)            validation_acc = float('{:.2f}'.format((list(acc_pred).count(True) / len(acc_pred)) * 100))            validation_accuracy.append(validation_acc)            if i == 1:                print(self.optimizer.name)            print(f"Epoch {i} :: Avg Train Loss = {float('{:.3f}'.format(mean(training_losses)))} : Avg Val Loss = {float('{:.3f}'.format(mean(validation_losses)))} : Avg Train Acc = {float('{:.2f}'.format(mean(training_accuracy)))} : Avg Val Acc = {float('{:.2f}'.format(mean(validation_accuracy)))}")            if i == self.epoch:                print('\n')        test_acc = self.prediction_acc(x_test, y_test)        self.history = {'Training_Loss': training_losses, 'Validation_Loss': validation_losses, 'Training_Accuracy': training_accuracy, 'Validation_Accuracy': validation_accuracy, 'Test_Accuracy': test_acc}    @staticmethod    def create_batch(X, Y, batch_size):        m = X.shape[0]        batches = []        num_batch = m / batch_size        for i in range(int(num_batch + 1)):            x_batch = X[i*batch_size:(i+1)*batch_size]            y_batch = Y[i*batch_size:(i+1)*batch_size]            batches.append((x_batch, y_batch))        if m % batch_size == 0:            batches.pop(-1)        return batches    @staticmethod    def logistic_loss(Y, Y_hat):        m = Y.shape[0]        L = (-1./m) * np.sum(Y * np.log(Y_hat))        return L    @staticmethod    def mse_loss(Y, Y_hat):        return np.mean((Y - Y_hat) ** 2)    def __repr__(self):        return str(self.layers)import tensorflow.keras.datasets.mnist as mnistdata = mnist.load_data()(xx_train, yy_train), (x_test, y_test) = dataxx_train = xx_train.reshape(xx_train.shape[0], -1)x_test = x_test.reshape(x_test.shape[0], -1)xx_train = np.array(xx_train/255., dtype=np.float32)x_test = np.array(x_test/255., dtype=np.float32)def one_hot(Y):    num_labels = len(set(Y))    new_Y = []    for label in Y:        encoding = np.zeros(num_labels)        encoding[label] = 1.        new_Y.append(encoding)    return np.array(new_Y)yy_train = one_hot(yy_train)y_test = one_hot(y_test)split_point = round(xx_train.shape[0] * 0.8)x_train = xx_train[:split_point, :]y_train = yy_train[:split_point, :]x_validation = xx_train[split_point:, :]y_validation = yy_train[split_point:, :]epoch = 100learning_rate = 1e-3batch_size = 128def Build_Model(optimizer=None):    model = Model()    model.add(Layer(128, activation='ReLU'))    model.add(Layer(64, activation='ReLU'))    model.add(Layer(10, activation='Softmax'))    model.set_config(epoch=epoch, learning_rate=learning_rate, optimizer=optimizer)    model.fit(x_train, y_train, x_test, y_test, batch_size=batch_size)    return model.historygd = OPT.GD(learning_rate=learning_rate, name='SGD')sgdm1 = OPT.SGDM1(learning_rate=learning_rate, name='SGDM1', beta=0.9)sgdm2 = OPT.SGDM2(learning_rate=learning_rate, name='SGDM2', ini_mo=0.5, fin_mo=0.99, demon=False, ini_beta=0.9)nesterov = OPT.Nesterov(learning_rate=learning_rate, name='Nesterov', ini_mo=0.5, fin_mo=0.99, demon=False, ini_beta=0.9)adagrad = OPT.Adagrad(learning_rate=learning_rate, name='Adagrad', epsilon=1e-8)rmsprop = OPT.RMSProp(learning_rate=learning_rate, name='RMSProp', decay_rate=0.9, epsilon=1e-8)adam = OPT.Adam(learning_rate=learning_rate, name='Adam', beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=False, gamma=1e-5, decay_rate=0.8, demon=False)optimizers = [gd, sgdm1, sgdm2, nesterov, adagrad, rmsprop, adam]optimizers_history = {i.name: Build_Model(i) for i in optimizers}epoch_list = [i for i in range(epoch)]for i in range(len(optimizers_history.keys())):    fig = plt.figure(i + 1, figsize=(10, 15))    fig.suptitle(f'Performance Analysis for {list(optimizers_history)[i]}', fontsize=8)    ax1 = fig.add_subplot(211)    ax2 = fig.add_subplot(212)    ax1.set_title('Analysis of Loss', fontsize=8, pad=3)    ax1.set_xlabel('Epoch', fontsize=8, labelpad=2)    ax1.set_ylabel('Loss', fontsize=8, labelpad=2)    ax1.tick_params(axis='x', labelsize=6, pad=0)    ax1.tick_params(axis='y', labelsize=6, pad=0)    ax1.plot(epoch_list, optimizers_history[list(optimizers_history)[i]]['Training_Loss'], color='r', label='Training Loss')    ax1.plot(epoch_list, optimizers_history[list(optimizers_history)[i]]['Validation_Loss'], color='g', label='Validation Loss')    ax1.legend(loc='best', prop={"size": 6})    ax2.set_title('Analysis of Accuracy', fontsize=8, pad=3)    ax2.set_xlabel('Epoch', fontsize=8, labelpad=2)    ax2.set_ylabel('Accuracy', fontsize=8, labelpad=2)    ax2.tick_params(axis='x', labelsize=6, pad=0)    ax2.tick_params(axis='y', labelsize=6, pad=0)    ax2.plot(epoch_list, optimizers_history[list(optimizers_history)[i]]['Training_Accuracy'], color='r')    ax2.plot(epoch_list, optimizers_history[list(optimizers_history)[i]]['Validation_Accuracy'], color='g')    red_patch = mpatches.Patch(color='white', label=f"Testing Accuracy = {optimizers_history[list(optimizers_history)[i]]['Test_Accuracy']}")    legend1 = ax2.legend(handles=[red_patch], loc='lower right', prop={"size": 7})    ax2.add_artist(legend1)    ax2.legend(['Training Accuracy', 'Validation Accuracy'], loc='best', prop={"size": 6})    plt.tight_layout(pad=5, h_pad=9)    plt.show()colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'orange']fig11 = plt.figure(len(optimizers_history.keys()) + 4, figsize=(10, 15))fig11.suptitle('Validation Accuracy for Different Optimizers', fontsize=8)ax11 = fig11.add_subplot(111)ax11.set_xlabel('Epoch', fontsize=8, labelpad=2)ax11.set_ylabel('Accuracy', fontsize=8, labelpad=2)ax11.tick_params(axis='x', labelsize=6, pad=0)ax11.tick_params(axis='y', labelsize=6, pad=0)fig10 = plt.figure(len(optimizers_history.keys()) + 3, figsize=(10, 15))fig10.suptitle('Training Accuracy for Different Optimizers', fontsize=8)ax10 = fig10.add_subplot(111)ax10.set_xlabel('Epoch', fontsize=8, labelpad=2)ax10.set_ylabel('Accuracy', fontsize=8, labelpad=2)ax10.tick_params(axis='x', labelsize=6, pad=0)ax10.tick_params(axis='y', labelsize=6, pad=0)fig9 = plt.figure(len(optimizers_history.keys()) + 2, figsize=(10, 15))fig9.suptitle('Validation Loss for Different Optimizers', fontsize=8)ax9 = fig9.add_subplot(111)ax9.set_xlabel('Epoch', fontsize=8, labelpad=2)ax9.set_ylabel('Loss', fontsize=8, labelpad=2)ax9.tick_params(axis='x', labelsize=6, pad=0)ax9.tick_params(axis='y', labelsize=6, pad=0)fig8 = plt.figure(len(optimizers_history.keys()) + 1, figsize=(10, 15))fig8.suptitle('Training Loss for Different Optimizers', fontsize=8)ax8 = fig8.add_subplot(111)ax8.set_xlabel('Epoch', fontsize=8, labelpad=2)ax8.set_ylabel('Loss', fontsize=8, labelpad=2)ax8.tick_params(axis='x', labelsize=6, pad=0)ax8.tick_params(axis='y', labelsize=6, pad=0)ii = 0for i, performance in optimizers_history.items():    ax8.plot(epoch_list, optimizers_history[i]['Training_Loss'], color=colors[ii], label=f'{i}')    ax9.plot(epoch_list, optimizers_history[i]['Validation_Loss'], color=colors[ii], label=f'{i}')    ax10.plot(epoch_list, optimizers_history[i]['Training_Accuracy'], color=colors[ii], label=f'{i}')    ax11.plot(epoch_list, optimizers_history[i]['Validation_Accuracy'], color=colors[ii], label=f'{i}')    ii += 1ax8.legend(loc='best', prop={"size": 6})ax9.legend(loc='best', prop={"size": 6})ax10.legend(loc='best', prop={"size": 6})ax11.legend(loc='best', prop={"size": 6})plt.show()